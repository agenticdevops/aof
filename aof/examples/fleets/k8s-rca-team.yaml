# Kubernetes RCA (Root Cause Analysis) Team Fleet
# A specialized fleet for diagnosing and analyzing failing pods in Kubernetes clusters
# Uses kubectl-ai MCP server for Kubernetes operations

apiVersion: aof.dev/v1
kind: AgentFleet
metadata:
  name: k8s-rca-team
  labels:
    team: sre
    category: incident-response

spec:
  agents:
    # Pod Status Analyzer - Initial triage using kubectl-ai
    - name: pod-analyzer
      role: worker
      replicas: 1
      spec:
        model: openai:gpt-4o
        instructions: |
          You are a Kubernetes Pod Status Analyzer. Your job is to:
          1. Analyze pod states (CrashLoopBackOff, ImagePullBackOff, Pending, etc.)
          2. Categorize failures by type and severity
          3. Identify patterns across multiple failing pods
          4. Provide initial triage assessment

          Use the kubectl-ai tools to gather pod information. Focus on:
          - Pod status and conditions using: kubectl get pods -n <namespace> -o wide
          - Container states: kubectl describe pod <pod-name> -n <namespace>
          - Recent events: kubectl get events -n <namespace> --sort-by='.lastTimestamp'

          Output a structured analysis with severity ratings (Critical, High, Medium, Low).
        mcp_servers:
          - name: kubectl-ai
            transport: stdio
            command: kubectl-ai
            args:
              - "--mcp-server"
              - "--mcp-server-mode=stdio"
            timeout_secs: 120
        max_iterations: 8

    # Log Investigator - Deep dive into logs using kubectl-ai
    - name: log-investigator
      role: worker
      replicas: 1
      spec:
        model: openai:gpt-4o
        instructions: |
          You are a Kubernetes Log Investigator. Your responsibilities:
          1. Examine container logs for error patterns
          2. Identify root causes from log messages
          3. Correlate errors across related pods
          4. Extract actionable error messages

          Use kubectl-ai tools to examine logs:
          - kubectl logs <pod-name> -n <namespace> --tail=100
          - kubectl logs <pod-name> -n <namespace> --previous (for crashed containers)
          - kubectl logs <pod-name> -c <container> -n <namespace> (for multi-container pods)

          Look for:
          - Stack traces and exceptions
          - Connection failures and timeouts
          - Resource exhaustion messages
          - Configuration and permission errors

          Provide specific log excerpts that indicate the root cause.
        mcp_servers:
          - name: kubectl-ai
            transport: stdio
            command: kubectl-ai
            args:
              - "--mcp-server"
              - "--mcp-server-mode=stdio"
            timeout_secs: 120
        max_iterations: 8

    # Resource & Metrics Analyst - Check resources and Prometheus metrics
    - name: resource-analyst
      role: worker
      replicas: 1
      spec:
        model: openai:gpt-4o
        instructions: |
          You are a Kubernetes Resource and Metrics Analyst. Your focus areas:
          1. Analyze resource requests and limits
          2. Check node capacity and scheduling constraints
          3. Query Prometheus for relevant metrics
          4. Identify resource bottlenecks

          Use kubectl-ai tools to check:
          - Pod resources: kubectl get pod <pod> -n <ns> -o jsonpath='{.spec.containers[*].resources}'
          - Node capacity: kubectl describe nodes | grep -A 5 "Allocated resources"
          - Pending reasons: kubectl describe pod <pod> -n <ns> | grep -A 10 "Events"
          - Top consumers: kubectl top pods -n <namespace>

          For Prometheus metrics, you can query via kubectl:
          - kubectl exec -n monitoring prometheus-prom-kube-prometheus-stack-prometheus-0 -c prometheus -- \
            wget -qO- 'http://localhost:9090/api/v1/query?query=<promql>'

          Key PromQL queries to consider:
          - container_memory_usage_bytes{namespace="<ns>"}
          - kube_pod_container_status_restarts_total{namespace="<ns>"}
          - kube_pod_status_phase{namespace="<ns>",phase!="Running"}

          Highlight any resource misconfigurations or capacity issues.
        mcp_servers:
          - name: kubectl-ai
            transport: stdio
            command: kubectl-ai
            args:
              - "--mcp-server"
              - "--mcp-server-mode=stdio"
            timeout_secs: 120
        max_iterations: 8

    # RCA Synthesizer - Combine findings into actionable report
    - name: rca-synthesizer
      role: manager
      replicas: 1
      spec:
        model: openai:gpt-4o
        instructions: |
          You are the RCA Synthesizer. Your job is to:
          1. Combine findings from all analysts into a cohesive report
          2. Determine the root cause(s) of failures
          3. Prioritize issues by impact and urgency
          4. Provide specific remediation steps

          Create a structured RCA report with these sections:

          ## Executive Summary
          Brief overview of the incident and key findings.

          ## Root Causes Identified
          List each root cause with supporting evidence.

          ## Impact Assessment
          - Number of affected pods
          - Services impacted
          - User-facing impact

          ## Remediation Steps (Priority Order)
          1. Immediate actions (Critical)
          2. Short-term fixes (High)
          3. Long-term improvements (Medium)

          Include specific kubectl commands for each remediation step.

          ## Prevention Recommendations
          How to prevent similar issues in the future.

          Be specific and actionable. Reference the namespace and pod names directly.
        tools: []
        max_iterations: 3

  # Peer coordination - all agents analyze in parallel, then synthesize
  coordination:
    mode: peer
    distribution: round-robin
    consensus:
      algorithm: majority
      min_votes: 3
      timeout_ms: 180000
      allow_partial: true

  # Communication for sharing findings
  communication:
    pattern: broadcast
    broadcast:
      channel: k8s-rca
      include_sender: true

  # Shared memory for cross-agent findings
  shared:
    memory:
      type: inmemory
      namespace: k8s-rca
      ttl: 1800
